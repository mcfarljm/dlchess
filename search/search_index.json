{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DLChess","text":"<p>Welcome to the documentation for the DLChess (deep learning for chess) project.</p> <p>DLChess is a UCI-compatible chess engine and reinforcement learning system.  It implements an AlphaZero-style model that combines Monte Carlo Tree Search (MCTS) with a neural network for policy and value prediction. The primary goal of the project is to explore how strong the program could become using a relatively small neural network together with selfplay and training on modest hardware.</p> <p>Note that the AlphaZero method uses self-play and reinforcement learning to train a neural network, without any domain knowledge or hand-crafted evaluation functions (e.g., the network starts out knowing nothing about piece values).  No external game data is used in the process of training the network.  This makes the training process very computationally demanding (the original work on AlphaZero by DeepMind employed thousands of GPUs).  However, this project shows that using smaller networks, training an intermediate strength model is possible even using just one, consumer-grade CPU.</p> <p>See the Design section for more information about the design and implementation of dlchess.  See Training Runs for information about the runs that have been performed so far to train networks.  There is also a page of Sample Games that gives a sense of how the networks play versus a human.</p>"},{"location":"sample-games/","title":"Sample Games","text":""},{"location":"sample-games/#v920-wins-as-black","title":"v9.20 wins as black","text":"<p>In this game I lost as white to network v9.20.  <code>dlchess</code> plays the odd King move <code>Kf7</code> early on, but there didn't seem to be an obvious way to attack it.  <code>dlchess</code> delivered a nice mate at the end.</p> <p> [Date \"2024.08.08\"] [White \"mcfarljm\"] [Black \"dlchess\"] [Result \"0-1\"] [TimeControl \"600\"] [Annotator \"1... -0.16\"] 1. d4 d5 {-0.16/5 4} 2. Nf3 e6 {-0.11/5 5} 3. Bf4 Nc6 {-0.01/5 5} 4. e3 f6 {-0.05/5 5} 5. h3 g5 {-0.05/6 5} 6. Bh2 h5 {-0.09/5 5} 7. c4 Kf7 {-0.13/5 6} 8. cxd5 exd5 {-0.04/5 6} 9. Nc3 Nh6 {-0.03/5 6} 10. Nb5 Qd7 {+0.03/5 6} 11. Nxc7 Rb8 {-0.02/6 6} 12. Rc1 g4 {+0.08/6 6} 13. hxg4 hxg4 {+0.08/6 6} 14. Nd2 Nf5 {+0.00/5 6} 15. Nb5 g3 {+0.34/7 6} 16. fxg3 Nxe3 {+0.31/8 6} 17. Qf3 Ng4 {+0.31/7 6} 18. Nc3 Rh5 {+0.25/8 7} 19. Qf4 Bd6 {+0.56/10 7} 20. Qf3 Nxd4 {+0.60/10 7} 21. Qd3 Qe6+ {+0.55/9 6} 22. Be2 Nxe2 {+0.52/8 7} 23. Nxe2 Rxh2 {+0.52/7 7} 24. Rxh2 Nxh2 {+0.73/6 6} 25. Qh7+ Ke8 {+0.59/7 6} 26. Qxh2 Bd7 {+0.43/5 6} 27. Kd1 b5 {+0.32/6 6} 28. Nf4 Qg4+ {+0.25/7 6} 29. Kc2 Rc8+ {+1.41/8 6} 30. Kb1 Qf5+ {+2.80/9 6} 31. Ka1 Rxc1+ {+28.10/3 5} 32. Nb1 Rxb1# {+128.00/1 5} {Xboard adjudication: Checkmate} 0-1 </p>"},{"location":"sample-games/#v920-loses-as-white","title":"v9.20 loses as white","text":"<p>In this game I won as black vs network v9.20.  <code>dlchess</code> played a relatively solid game, but seemed to mis-evaluate a somewhat complicated position that occurred in the middlegame.</p> <p> [Date \"2024.08.04\"] [White \"dlchess\"] [Black \"mcfarljm\"] [Result \"0-1\"] 1. e4 {EV: 55.9%, N: 58.41% of 20.0k} c6 2. d4 {EV: 57.9%, N: 34.23% of 20.0k} d5 3. c4 {EV: 58.7%, N: 45.63% of 20.0k} dxe4 4. Nc3 {EV: 57.9%, N: 31.50% of 20.0k} Nf6 5. f3 {EV: 58.0%, N: 56.45% of 20.0k} exf3 6. Nxf3 {EV: 58.4%, N: 82.56% of 20.0k} Bf5 7. h3 {EV: 62.1%, N: 56.92% of 20.0k} h6 8. g4 {EV: 63.8%, N: 64.63% of 20.0k} Bg6 9. Qe2 {EV: 65.2%, N: 18.54% of 20.0k} e6 10. Ne5 {EV: 64.6%, N: 43.58% of 20.0k} Bh7 11. Nf3 {EV: 64.7%, N: 65.73% of 20.0k} Be7 12. b3 {EV: 66.6%, N: 46.62% of 20.0k} c5 13. dxc5 {EV: 67.7%, N: 33.48% of 20.0k} Bxc5 14. Bb2 {EV: 66.7%, N: 51.76% of 20.0k} O-O 15. a3 {EV: 69.3%, N: 18.30% of 20.0k} Re8 16. Qd1 {EV: 69.3%, N: 29.11% of 20.0k} Nc6 17. b4 {EV: 67.7%, N: 46.23% of 20.0k} Bd6 18. Rc1 {EV: 66.9%, N: 22.77% of 20.0k} Bg3+ 19. Ke2 {EV: 60.2%, N: 100.00% of 20.0k} Rc8 20. Qd2 {EV: 68.3%, N: 28.64% of 20.0k} Ne7 21. Ba1 {EV: 69.3%, N: 35.08% of 20.0k} Rxc4 22. Qxd8 {EV: 57.8%, N: 11.28% of 20.0k} Rxd8 23. Nd2 {EV: 53.5%, N: 41.77% of 20.0k} Bf4 24. Rd1 {EV: 55.9%, N: 47.95% of 20.0k} Rcc8 25. Bb2 {EV: 55.3%, N: 16.59% of 20.0k} Nfd5 26. Kf3 {EV: 50.9%, N: 26.59% of 20.0k} Nxc3 27. Bxc3 {EV: 38.2%, N: 92.77% of 20.0k} Ng6 28. Bb2 {EV: 43.9%, N: 20.45% of 20.0k} Bxd2 29. Rh2 {EV: 35.1%, N: 24.53% of 20.0k} Bf4 30. Rc2 {EV: 34.6%, N: 59.62% of 20.0k} Nh4+ 31. Kxf4 {EV: 30.7%, N: 97.30% of 20.0k} Bxc2 32. Rxd8+ {EV: 30.2%, N: 60.67% of 20.0k} Rxd8 33. Ke3 {EV: 29.6%, N: 53.29% of 20.0k} f5 34. Be5 {EV: 28.1%, N: 29.34% of 20.0k} Ng6 35. Bc7 {EV: 25.9%, N: 31.00% of 20.0k} Rd7 36. Bb8 {EV: 30.0%, N: 75.31% of 20.0k} a6 37. Be2 {EV: 29.4%, N: 25.18% of 20.0k} Bd1 38. gxf5 {EV: 30.0%, N: 32.44% of 20.0k} exf5 39. Bc4+ {EV: 29.1%, N: 56.22% of 20.0k} Kh7 40. b5 {EV: 28.9%, N: 26.82% of 20.0k} axb5 41. Bxb5 {EV: 28.1%, N: 90.01% of 20.0k} Rd5 42. Bc4 {EV: 27.4%, N: 34.33% of 20.0k} Ra5 43. Bd6 {EV: 25.7%, N: 32.59% of 20.0k} b5 44. Bd3 {EV: 28.5%, N: 56.98% of 20.0k} Nh4 45. Bb4 {EV: 30.4%, N: 82.54% of 20.0k} Ra8 46. Bxb5 {EV: 29.5%, N: 34.38% of 20.0k} g5 47. Bc6 {EV: 31.7%, N: 20.38% of 20.0k} Rb8 48. Kd2 {EV: 30.8%, N: 31.39% of 20.0k} Rd8+ 49. Ke1 {EV: 28.6%, N: 50.22% of 20.0k} Bc2 50. Be7 {EV: 29.1%, N: 65.19% of 20.0k} Rd3 51. Ke2 {EV: 28.6%, N: 29.02% of 20.0k} Rxh3 52. Bd7 {EV: 27.0%, N: 36.65% of 20.0k} Rh2+ 53. Ke3 {EV: 17.7%, N: 58.00% of 20.0k} f4+ 54. Kd4 {EV: 14.8%, N: 100.00% of 20.0k} f3 55. Bf8 {EV: 19.8%, N: 14.99% of 20.0k} f2 56. Bb5 {EV: 18.4%, N: 78.03% of 20.0k} Nf3+ 57. Ke3 {EV: 23.4%, N: 86.31% of 20.0k} Ne5 58. Bf1 {EV: 23.4%, N: 32.91% of 20.0k} Bd3 59. Bxd3+ {EV: 18.5%, N: 64.85% of 20.0k} Nxd3 60. Ke2 {EV: 15.7%, N: 75.19% of 20.0k} Rh1 61. Kxd3 {EV: 9.2%, N: 22.64% of 20.0k} f1=Q+ 62. Ke3 {EV: 7.0%, N: 24.07% of 20.0k} Qxf8 63. a4 {EV: 5.4%, N: 49.37% of 20.0k} Qb4 64. a5 {EV: 5.6%, N: 29.75% of 20.0k} Qxa5 65. Kf3 {EV: 6.0%, N: 38.81% of 20.0k} Qb4 66. Kg2 {EV: 6.9%, N: 51.46% of 20.0k} Ra1 67. Kg3 {EV: 6.4%, N: 24.77% of 20.0k} Ra3+ 68. Kg2 {EV: 4.5%, N: 37.48% of 20.0k} Qb2+ 69. Kf1 {EV: 1.3%, N: 34.03% of 20.0k} Ra1# * </p>"},{"location":"sample-games/#wild-draw-vs-v910","title":"Wild draw vs v9.10","text":"<p>Below is a game that I played as white vs network v9.10.  <code>dlchess</code> sacrifices a knight early on and then makes it back up later by winning two pieces with a double pawn-fork. I missed what should have been an easy conversion in the end with two pawns, after <code>dlchess</code> again gave up another knight for seemingly no reason on move 38.</p> <p> [Event \"Computer Chess Game\"] [Date \"2024.07.28\"] [White \"mcfarljm\"] [Black \"dlchess\"] [Result \"1/2-1/2\"] [TimeControl \"600\"] [Annotator \"1... +0.19\"] 1. d4 h5 {+0.19/5 4} 2. e4 b5 {+0.28/4 5} 3. Bxb5 c5 {+0.39/4 5} 4. Nf3 Qb6 {+0.37/4 5} 5. Be2 cxd4 {+0.32/5 5} 6. Qxd4 Qxd4 {+0.53/6 5} 7. Nxd4 g6 {+0.51/5 6} 8. O-O Nf6 {+0.51/4 6} 9. Nc3 Nxe4 {+0.38/5 6} 10. Nxe4 d5 {+0.26/5 6} 11. Nc3 Bg7 {+0.23/5 6} 12. Be3 Na6 {+0.04/5 6} 13. a3 e5 {+0.20/5 6} 14. Nf3 d4 {+0.12/6 6} 15. Nxd4 exd4 {+0.68/7 6} 16. Bxd4 Bxd4 {+0.70/7 6} 17. Rad1 Bxc3 {+0.81/6 6} 18. bxc3 Nc5 {+0.79/5 7} 19. Rfe1 Be6 {+0.80/5 6} 20. Bf3 Rd8 {+0.66/5 6} 21. Rb1 Rc8 {+0.80/4 6} 22. Bd5 Na4 {+1.01/5 6} 23. Bxe6 fxe6 {+0.83/7 6} 24. Rxe6+ Kf7 {+0.72/7 6} 25. Ra6 Nxc3 {+0.64/6 6} 26. Rxa7+ Ke6 {+0.62/5 6} 27. Rb6+ Kf5 {+0.50/5 6} 28. h3 Ne2+ {+0.50/5 6} 29. Kh2 Rhg8 {+0.62/4 6} 30. Rf7+ Kg5 {+0.46/6 6} 31. a4 Rcf8 {+0.42/5 5} 32. Rxf8 Rxf8 {+0.31/6 5} 33. f3 Ra8 {+0.37/4 5} 34. Rb4 Nc3 {+0.37/5 5} 35. Rd4 Rxa4 {+0.50/5 5} 36. Rd3 Ne2 {+0.50/5 5} 37. Re3 Nf4 {+0.48/6 5} 38. g3 Ra7 {+0.47/7 5} 39. gxf4+ Kxf4 {+0.44/6 4} 40. Rb3 Rg7 {+0.51/4 4} 41. c4 Ke5 {+0.49/5 4} 42. c5 Kd5 {+0.58/5 4} 43. Rc3 Kd4 {+0.46/5 4} 44. Rc1 Rc7 {+0.35/5 4} 45. Kg3 Kd5 {+0.29/5 3} 46. Kf4 h4 {+0.17/5 3} 47. Kg5 Rxc5 {+0.05/5 3} 48. Rxc5+ Kxc5 {-0.07/7 3} 49. Kxg6 Kd6 {-0.03/6 2.9} 50. f4 Ke7 {-0.20/9 2.8} 51. f5 Kf8 {-0.35/11 2.8} 52. f6 Kg8 {-0.31/12 2.6} 53. Kg5 Kf7 {-0.11/9 2.5} 54. Kf5 Kf8 {-0.38/13 2.4} 55. Kg4 Kf7 {-0.47/12 2.3} 56. Kxh4 Kxf6 {-0.13/8 2.1} 57. Kh5 Kf7 {-0.15/8 2.0} 58. Kh6 Kf6 {-0.20/8 1.8} 59. h4 Kf7 {-0.24/8 1.7} 60. Kh7 Kf8 {-0.24/9 1.6} 61. h5 Kf7 {-0.21/10 1.5} 62. h6 Kf8 {-0.13/10 1.3} 63. Kg6 Kg8 {-0.08/8 1.3} 64. Kh5 Kh7 {-0.06/7 1.2} 65. Kg5 Kh8 {-0.06/7 1.1} 66. Kg6 Kg8 {-0.05/7 1.0} 67. Kf6 Kh7 {-0.03/5 0.9} 68. Kg5 Kh8 {-0.02/5 0.9} 69. Kg6 Kg8 {+0.00/1 0.7} {Draw by repetition} 1/2-1/2 </p>"},{"location":"sample-games/#v99-wins-with-black","title":"v9.9 wins with black","text":"<p>Below is a sample game vs network v9.9, where I lost with White.  This one is a little embarassing, as <code>dlchess</code> sacked a Knight for a Pawn early in the game and still won.</p> <p> [Date \"2024.07.28\"] [White \"Human\"] [Black \"dlchess\"] [Result \"0-1\"] [GameDuration \"00:07:18\"] [GameEndTime \"2024-07-28T10:53:23.254 MDT\"] [GameStartTime \"2024-07-28T10:46:04.791 MDT\"] [Opening \"Queen's pawn\"] [PlyCount \"64\"] [TimeControl \"40/300\"] 1. d4 a5 {+0.14/4 4.4s} 2. e4 {4.1s} g6 {+0.18/4 4.6s} 3. Nf3 {4.3s} Bh6 {+0.23/4 4.8s} 4. Bxh6 {6.6s} Nxh6 {+0.17/5 5.0s} 5. c4 {1.7s} Na6 {+0.21/4 5.2s} 6. Nc3 {2.7s} Ng4 {+0.20/4 5.4s} 7. Bd3 {8.8s} Nxf2 {+0.30/5 5.6s} 8. Kxf2 {2.9s} Nb4 {+0.30/5 5.8s} 9. Be2 {18s} h5 {+0.36/5 5.9s} 10. a3 {1.4s} Na6 {+0.42/5 6.0s} 11. Re1 {6.9s} d6 {+0.50/4 6.1s} 12. Rc1 {9.6s} c6 {+0.39/4 6.2s} 13. e5 {20s} dxe5 {+0.43/5 6.3s} 14. Nxe5 {5.3s} Bf5 {+0.56/4 6.4s} 15. Bd3 {9.1s} Qxd4+ {+0.66/6 6.4s} 16. Kf1 {26s} Bxd3+ {+0.52/6 6.5s} 17. Qxd3 {3.8s} Qxd3+ {+0.44/6 6.5s} 18. Nxd3 {1.8s} Rd8 {+0.42/5 6.6s} 19. Rcd1 {4.1s} Rd6 {+0.53/4 6.4s} 20. b4 {3.5s} Rd4 {+0.43/6 6.4s} 21. b5 {12s} Nb8 {+0.42/7 6.3s} 22. Nb2 {16s} Rf4+ {+0.56/5 6.3s} 23. Kg1 {2.7s} cxb5 {+0.48/5 6.4s} 24. cxb5 {3.6s} Rg4 {+0.41/5 6.3s} 25. a4 {4.9s} f6 {+0.37/5 6.1s} 26. Rd5 {13s} Kf7 {+0.48/5 6.0s} 27. Red1 {6.6s} Rc8 {+0.71/5 5.9s} 28. Rd8 {4.1s} Rxc3 {+0.40/7 5.8s} 29. Rxb8 {7.3s} Rc2 {+0.54/7 5.8s} 30. Nd3 {24s} Rgxg2+ {+1.01/10 5.7s} 31. Kf1 {7.4s} Rxh2 {+1.07/9 5.6s} 32. Rxb7 {3.2s} Rh1# {+128.00/1 5.3s, Black mates} 0-1 </p> <p>PGN viewer powered by Chess Tempo: https://chesstempo.com/ </p>"},{"location":"training-runs/","title":"Training Runs","text":"<p>This page documents and compares different neural networks that have been trained through reinforcement learning.  Each training run starts from a randomly initialized neural network and iterates between selfplay and training updates.  All training runs were conducted using modest hardware with between 4 and 8 CPU cores.</p>"},{"location":"training-runs/#results","title":"Results","text":"<p>The following summarizes results for different training runs.  The naming scheme is <code>v{MAJOR_VER}.{INCREMENT}</code>, where <code>MAJOR_VERSION</code> indicates the training run number, and <code>{INCREMENT}</code> denotes the number of training updates that have been made within that training run.  For example, <code>v4.0</code> corresponds to the randomly initialized network for training run 4, and <code>v4.5</code> corresponds to the network in training run 4 after 5 updates.</p> <p>The table below gives a summary for a selected set of training runs.  \"Size\" specifies the size of the neural network architecture in terms of number of blocks and number of filters (e.g., 4 blocks with 64 filters).  \"Games\" refers to the total number of selfplay games used for training.  \"ELO\" is based on comparison against other engines with similar strength from the CCRL blitz ratings in 2+1 blitz matches.</p> Network Size Block Type Games ELO Comments v14.40 5 x 64 Squeeze Excitation 128,000 1380 Increased number of blocks v12.25 4 x 64 Squeeze Excitation 80,000 1263 Added squeeze-excitation layer to residual blocks v11.25 4 x 64 Residual 80,000 1192 Board oriented towards side to move v9.20 4 x 64 Residual 64,000 1042 First run with residual blocks v8.15 4 x 64 Convolution 48,000 820 Added en passant square to network input; updated parameters v4.15 4 x 64 Convolution 48,000 789 First successful training run <p>The below plots show estimated strength as a function of training history.  Note that the y-axis is a relative ELO, which is set to 0 for the randomly initialized network. Relative ELO is calculated after each training update by running a 200-game tournament between the new network and the previous version.  In this tournament, both models play each side from 100 different standard opening positions.  Search is fixed at 800 playouts.</p> Training history for network v14.40 Training history for network v12.25 Training history for network v11.25 Training history for network v9.20 Training history for network v8.15 Training history for network v4.15"},{"location":"training-runs/#settings","title":"Settings","text":""},{"location":"training-runs/#move-selection","title":"Move selection","text":"<ul> <li>800 MCTS rounds are used during self-play and model evaluation.</li> <li>Dirichlet noise is added to the root node with a concentration parameter equal to $0.03 \\times 19 \\times 19 / n_{moves}$, following Katago.</li> <li>Self-play<ul> <li>Network v5 and later: all moves are selected randomly in proportion to visit count.</li> <li>Network v4 and earlier: the first 15 full moves (30 half-moves) are selected randomly in proportion to visit counts, and remaining moves are selected greedily based on maximum visit count.</li> </ul> </li> <li>Tree search is done in serial.  During self-play, parallelization is accomplished by running multiple processes.</li> </ul>"},{"location":"training-runs/#self-play","title":"Self-play","text":"<ul> <li>Self-play games were assigned a draw outcome if the game exceeded 256 half-moves.</li> <li>Each iteration consisted of 3,200 self-play games, after which a training update was performed.</li> </ul>"},{"location":"training-runs/#training","title":"Training","text":"<ul> <li>Training batch size was 256 moves.</li> <li>For each training iteration, all move data generated during the previous self-play   iteration were used.  For example, supposing 100 moves per game, we have $100 \\times   3,200 = 320,000$ moves for training.  With a batch size of 256, this corresponds to   roughly 1,250 training steps.</li> <li>Stochastic gradient descent was used with a learning rate of 5e-3.</li> </ul>"},{"location":"training-runs/#evaluation","title":"Evaluation","text":"<ul> <li>Strength was evaluated relative to the previous version after each training iteration.</li> <li>During evaluation, Dirichlet noise was disabled and all moves were selected greedily   based on visit count.  Evaluation used 800 playouts (the same as selfplay).</li> <li>200 games were played, with each player playing both sides from 100 standard opening   positions after 6 ply.</li> <li>Evaluation games were limited to 150 full moves, after which they were adjudicated a draw.</li> <li>Evaluation was performed using cutechess-cli.</li> </ul>"},{"location":"training-runs/#hardware","title":"Hardware","text":"<p>Training runs beginning with v9 were done using a refurbished Lenovo ThinkCentre mini pc with an Intel Core i7 having 4 cores and 8 threads.  Some initial issues with overheating had to be resolved by limiting the clock speed to 2.7 GHz.  Selfplay was done using the serial MCTS implementation with 4 self-play processes running concurrently, each using a single thread for network inference.  Due to the use of a cache for network inference, throughput increases over the course of model training.  At iteration v9.5, throughput was about 2.83 moves per second per process, for a total of approximately 11.32 moves per second.  A typical self-play iteration consisting of 800 games per worker (3,200 total) took about 10 hours, followed by an hour or so for the training update and strength evaluation.</p> <p>Earlier training runs were done on an Amazon EC2 instance with 8 virtual CPUs.  To obtain the best self-play throughput with the serial MCTS implementation, 8 self-play processes were run concurrently, and neural network model evaluation for each process was configured to use one thread.  Typical throughput was approximately 2.35 moves per second per process, for a total of approximately 18.8 moves per second (this was before network caching was implemented in dlchess).  A typical self-play iteration consisting of 400 games per worker (3,200 total) took about 7 hours.  After generating self-play data, the training update was fast, taking on the order of minutes.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"design/","title":"Design","text":"<p>This section provides information about the design and implementation of DLChess, including othe search algorithm and neural network architecture.</p>"},{"location":"design/alpha-zero/","title":"AlphaZero Search","text":""},{"location":"design/alpha-zero/#algorithm","title":"Algorithm","text":"<p>The AlphaZero search algorithm is a modified version of Monte Carlo tree search that incorporates a neural network.  The neural network expresses a mapping between an encoding of the game state and two outputs: (a) a value that measures probability of win, and (b) a policy vector that assigns probabilities to moves.  The key differences from standard Monte Carlo tree search are:</p> <ol> <li>In the \"Simulation\" (\"playout\") step, instead of simulating a game from the given    position, the value of the neural network is used as the playout result.</li> <li>In the selection step, the policy output from the network is used as a sort of    \"prior\" in a modified version of the PUCT algorithm.  The idea is to encourage the    selection of nodes with high policy, high value, and low visit count.</li> </ol> <p>The algorithm is implemented as a tree structure made up of nodes, where each node represents a particular game state.  Each node has a set of \"branches\" that represent the legal moves that can be played in that position.  When a branch is selected for the first time, this leads to a new game state and the creation of a new node.</p> <p>The diagram below depicts a simple tree with four nodes.  The \"Root\" node corresponds to the start of the game.  From this starting position, the search would begin by querying the neural network using an encoding of this game state as input.  Based on the PUCT formula, one of the possible moves would be selected.  Because no branches have been visited yet, the first selection would be governed by the move that has the highest policy from the neural network.  Suppose that this was the move 1.Na3.  The <code>Board::make_move</code> method (see Chess Framework) is used to create a new game state, and that new game state is used to instantiate a new node in the tree. When the new node is created, the neural network is evaluated using that state as input to obtain a corresponding value and policy.  This completes the first \"playout\" in the search.</p> <pre><code>graph TB\n  Root --&gt; |Na3|m1[1.Na3]\n  Root --&gt; |e4|m2[1.e4]\n  m2 --&gt; |e5|m3[1.e4 e5]</code></pre> <p>The search would then continue with the second playout.  We return to the root node and use the PUCT algorithm to select a move.  Now that the visit count for the Na3 move has changed, the PUCT algorithm may prioritize a different move; suppose that in the second playout, the move selected by PUCT is e4.  Since this move has not yet been visited, we again use <code>Board::make_move</code> to create a new game state, instantiate a new node, and query the network for value and policy.  This completes the second playout, and we return to the root.</p> <p>For the third playout, we again use the PUCT algorithm to select among the moves at the Root node.  Suppose that just like in the second round, it selects the move e4.  Since this branch has already been visited, we traverse to the 1.e4 node and apply the PUCT selection algorithm to select a move at that node.  Suppose now that the algorithm selects the move e5 (which is now a move played by the player with the black pieces).  This branch has not yet been visited, so it results in the creation of a new node 1.e4 e5 and a new neural network evaluation.</p> <p>Once the playouts are complete, the move is selected among the branches at root based on their numbers of visit counts.  Typically, the branch with the largest visit count is selected, but randomization can be introduced during self-play by selecting the move randomly in proportion to the number of visit counts.</p> <p>Some things to note about this process:</p> <ul> <li>For each playout in the search, the tree is traversed using branch selection until we   either arrive at a node that has not yet been visited or we arrive at a terminal node.   This means that not counting terminal nodes, every playout corresponds to one   evaluation of the neural network.</li> <li>At the end of each playout, the value obtained from the neural network (or the   terminal game value of 1, -1, or 0) is backpropagated to all parent branches (flipping   sign at each step to maintain appropriate perspective).  In this way, each branch   maintains a running average of values from all leaf nodes that traversed through it.   In our example with three playouts, the e4 branch from the root node has been   visited twice, and its value is the average of the neural network value predictions   from the 1.e4 and 1.e4 e5 nodes.</li> </ul>"},{"location":"design/alpha-zero/#data-structures","title":"Data Structures","text":"<p>The fundamental data structures used in the AlphaZero search method are the node and the branch.  The node records information about a particular location in the search tree, which includes the game state, the corresponding neural network output, visit count, and more.  The data members of the <code>ZeroNode</code> class are shown below.</p> ZeroNode data<pre><code>class ZeroNode {\npublic:\n  chess::Board game_board;\n  std::weak_ptr&lt;ZeroNode&gt; parent;\n  std::optional&lt;chess::Move&gt; last_move;\n  std::unordered_map&lt;chess::Move, std::shared_ptr&lt;ZeroNode&gt;, chess::MoveHash&gt; children;\n  std::unordered_map&lt;chess::Move, Branch, chess::MoveHash&gt; branches;\n  // Value from neural net, or true terminal value.\n  float value;\n  int total_visit_count = 1;\n  // Running average of expected value of child branches.\n  float expected_value_ = 0.0;\n  bool terminal;\n};\n</code></pre> <p>The <code>Branch</code> class contains information associated with a legal move that can be made from the game state of a particular <code>Node</code>.  The <code>Branch</code> class has a simple set of data members, as shown below.  Note that the <code>ZeroNode</code> class stores branches as part of a mapping from <code>chess::Move</code> to <code>Branch</code> instances.  Here <code>prior</code> is the neural network policy output corresponding to a particular branch.</p> Branch data<pre><code>class Branch {\npublic:\n  float prior;\n  int visit_count = 0;\n  float total_value = 0.0;\n};\n</code></pre>"},{"location":"design/chess-framework/","title":"Chess Framework","text":"<p>This section introduces the chess framework that <code>dlchess</code> is built on.  The framework provides the capabilities for representing the state of the board, enumerating all legal moves, apply/undoing moves, determining the game termination state, etc.  Ideally, we want the implementation to be very efficient, as we want to minimize all overhead associated with the chess framework.</p> <p>The goal of this section is to highlight some of the key functions that the deep learning system interacts with, and we will also review some of the lesser-known chess rules, which also come into play in the implementation of the search algorithms.</p> <p>The chess framework used by dlchess was written by the author in C++ and is adapted from the design detailed in the outstanding video tutorial series, VICE.  The key data structures that we care about are:</p> <ul> <li><code>chess::Board</code>: The main data structure, which describes the state of the game (board)   at a given point in time, including the complete move history.</li> <li><code>chess::Move</code>: A compact representation of a move, which contains the \"from\" and \"to\"   squares, piece capture information, promotion information, and a flag for en passant,   castling, and pawn starts.</li> </ul> <p>The key functions that are  used within the search code are:</p> <ul> <li><code>Board::make_move(Move)</code>: Update the board state in place by making the given move.   Note that although our chess framework also provides a corresponding   <code>Board::undo_move</code> function, we won't be using it in the deep learning implementation.   Instead, as we expand our search tree, we will first copy the current <code>Board</code> state   and then apply the move.</li> <li><code>Board::generate_legal_moves()</code>: Return a vector of <code>Move</code> instances for all legal   moves from a given <code>Board</code> position.</li> <li><code>Board::is_over()</code>: Return a bool indicating whether the game represented by the given   state is over.</li> <li><code>Board::winner()</code>: Return an <code>optional</code> instance that is either null (if the game is   not over) or indicates the game outcome (either one side wins or the game is drawn).</li> </ul> <p>In order for the chess framework to be able to correctly enumerate all moves and identify game termination, it must represent all rules, including some of the more esoteric rules of the game.  These include:</p> Castling <p>Each player's king has the possibility of making two different castling moves, which are possible under certain conditions.  In particular, the king cannot castle while in check, and castling privileges are lost once the king moves or a given rook moves.  Thus, the <code>chess::Board</code> structure must keep track of the state of all four castling permissions.</p> En passant <p>This is a special pawn capture move that is only allowed in a special circumstance where the opponent's pawn has moved forward two squares.  Importantly, the en passant capture is only available on the next move.  Thus, the <code>chess::Board</code> structure must encode the status of whether en passant capture is available, and if so, on which square.</p> Three-fold repetition <p>Three-fold repetition: If the same position occurs on the board three times in total (not necessarily consecutive), then the game terminates in a draw.  The chess framework handles this by using a position hash.  A hash function encodes each position into a large integer value.  The <code>chess::Board</code> structure keeps track of the hash of every previous position, which enables it to check for three-fold repetition.</p> Fifty-move rule <p>If fifty full moves (one move from each side) are played without a capture or a pawn advance, then the game ends in a draw.  Again, the <code>chess::Board</code> structure must keep track of the fifty-move count.</p>"},{"location":"design/neural-network/","title":"Neural Network","text":""},{"location":"design/neural-network/#architecture","title":"Architecture","text":"<p>The neural network architecture follows that of the original AlphaZero work, as shown below.  The architecture is made up of a \"base/tower\" of residual blocks, which is shared by both the policy and value outputs.  Following the base, there is a \"policy head\" and \"value head\".  The network is parameterized in terms of the number of blocks in the base, and the number of filters.  The original AlphaZero work used something like 19 residual blocks and 256 filters.  See Training Runs for information about the architecture settings that have been tried with <code>dlchess</code>. Details of the implementation, which uses PyTorch, can be seen in the source for resid_net.py.</p> <pre><code>graph TB\n  Input{Input} --&gt; |22 x 8 x 8|b1[Residual Block 1]\n  b1 --&gt; |filters x 8 x 8|bn[Residual Block n]\n  bn --&gt; pc1[Convolution]\n  pc1 --&gt; |filters x 8 x 8|pc2[Convolution]\n  pc2 --&gt; |73x 8 x 8|policy{Policy}\n  bn --&gt; vc1[Convolution, 1x1]\n  vc1 --&gt; |1 x 8 x 8|vl1[Linear]\n  vl1 --&gt; |256|vl2[Linear]\n  vl2 --&gt; |1|tanh[Tanh]\n  tanh --&gt; Value{Value}</code></pre> <p>The above diagram shows the use of residual blocks to construct the base; earlier versions of <code>dlchess</code> used simple convolution blocks instead, which results in a more compact network.</p>"},{"location":"design/neural-network/#input-encoding","title":"Input Encoding","text":"<p>The game state must be represented in a form that the neural network can process.  This is done by encoding the game state into a tensor, which can be fed into the network. dlchess uses a simple encoding that is based on the original AlphaZero work, but without the use of history.</p> <p>The building block of the input encoding is the concept of a plane.  A plane is an 8 by 8 array that represents some particular feature of the game.  The elements of the plane correspond to the squares on the chess board.  dlchess uses 22 input planes<sup>1</sup>:</p> <ul> <li>12 planes that encode piece occupation.  For each piece type, the corresponding plane   is set to 1 where a piece exists and 0 otherwise.</li> <li>2 planes that encode the repetition count (which is relevant for the three-fold   repetition rule).  The first plane is set to all ones if the repetition count is one   or greater, and zeros otherwise.  The second plane is set to all ones if the   repetition count is two or greater, and zeros otherwise.</li> <li>1 plane for color of side to move: set to all ones if black is to move, zeros   otherwise.</li> <li>4 planes for castling permissions, each set to all ones if the associated castling   permission is available.</li> <li>1 plane that encodes the no-progress count (relevant for the fifty-move rule).  This   plane is filled with the numerical value of the number half-moves that count towards   the fifty-move rule.</li> <li>1 plane that encodes the en passant square, if any.</li> <li>1 constant plane that is set to all ones, presumably to help with edge detection.</li> </ul> <p>Prior to encoding, the board is oriented towards the side to move (i.e., so that pawns for the side to move always move in the positive direction).</p> <p>The input encoder is designed against the following interface, where <code>Tensor</code> is a simple user-defined template that wraps <code>std::vector</code> with shape and stride information.</p> <pre><code>class Encoder {\npublic:\n  virtual Tensor&lt;float&gt; encode(const chess::Board&amp;) const = 0;\n};\n</code></pre>"},{"location":"design/neural-network/#output-decoding","title":"Output Decoding","text":"<p>A convention is required for how to map the neural network policy output, which is a tensor, to moves.  The challenge is that the space of legal moves in chess varies depending on the game state, and the different pieces move in different ways, including special moves such as castling, promotion, and en passant.  The idea is that the network policy output encompasses all possible moves in any position, and then we can filter on position-specific legal moves later.</p> <p>dlchess uses the same approach as AlphaZero, where the space of possible moves is represented by an 73x8x8 tensor, where only a subset of the elements correspond to possible moves.  To work with this mapping, we define a function that takes a game state as input and returns the indices of the legal moves, within the network output tensor. The interface for this function is:</p> <pre><code>std::unordered_map&lt;chess::Move, std::array&lt;int,3&gt;, chess::MoveHash&gt;\ndecode_legal_moves(const chess::Board&amp;);\n</code></pre> <p>The output of this function is a mapping from a <code>Move</code> to a length-3 array of indices, which indexes into the 73x8x8 policy tensor.  The length of the mapping that the function returns is the number of legal moves for the given game state.  For example, suppose that there is a particular position with only two legal moves.  Calling <code>decode_legal_moves</code> would return a map with two items: the keys define the <code>Move</code> objects for the two moves, and the values are index arrays that tell us the corresponding locations within the policy tensor.  For each move, we can use the indices to look up and extract the corresponding network policy.</p> <p>The details of how the function works are a little bit involved and can be seen by reading the code in encoder.cpp.</p>"},{"location":"design/neural-network/#inference-backend","title":"Inference Backend","text":"<p>The design of <code>dlchess</code> is such that the neural network architecture and training code are written in Python using PyTorch, but the search is written in C++.  Thus, we need to be able to run network inference from C++.  The original version of <code>dlchess</code> used TorchScript to run inference from C++. However, the current version uses ONNX Runtime, which resulted in a roughly 2.5x total speedup in search throughput using CPUs.</p> <p>The implementation of neural network inference using ONNX Runtime in <code>dlchess</code> can be seen here: inference.h.  This largely follows the structure of the ONNX Runtime C++ tutorials.  The <code>InferenceModel</code> class is instantiated from a path to the saved ONNX model file, and inference is wrapped using the <code>operator()</code> method, which accepts an input tensor and returns two output tensors.</p>"},{"location":"design/neural-network/#caching","title":"Caching","text":"<p>Other than switching from TorchScript to ONNX Runtime for inference, the biggest performance improvement for search throughput (which governs how long it takes to generate selfplay data) was the implementation of a cache for the neural network output. The idea is that we keep a record of previously evaluated positions, so that we can avoid running the same network inference again in the future.  There are a couple of reasons that we might see the same network inputs occur during search:</p> <ol> <li>Transpositions: The same position can be reached via different move orders.</li> <li>Selfplay: During selfplay, the same network is playing both sides of the match, which    means that positions searched by one side will frequently appear when the other side    is searching on the next move.  Similarly, since <code>dlchess</code> does not try to re-use the    search tree, previously evaluated positions may appear in later searches even if the    agent is only playing one side.</li> </ol> <p><code>dlchess</code> uses a fixed-size, first-in first-out hash map to store the network inference cache.  This is implemented using a class that contains a <code>std::unordered_map&lt;uint64_t, NetworkOutput&gt;</code> and a <code>std::queue&lt;uint64_t&gt;</code>.  The map stores previously computed network outputs based on a hash key of the input.  The queue keeps track of which keys (positions) should be evicted once the maximum size is reached.</p> <p>There is a small subtlety in defining the keys.  Nominally, the network input is an encoding of a game position.  One might think of using the standard Zobrist position hash key (which is already available in the <code>chess::Board</code> structure) as the network input hash key.  However, this is not correct because the Zobrist hash does not account for the repetition count or the fifty-move count, both of which are part of the network input encoding.  As such, these two values are hashed and concatenated together with the Zobrist position hash to produce the hash key for the network input.</p> <p>The degree to which this reduces the need to query the neural network can be significant.  For a trained network, the proportion of positions that can be found in the cache (even when using a modest cache size) may be 25% or more.  This value tends to be higher for trained networks, as untrained networks tend to have a less structured, more random search.</p>"},{"location":"design/neural-network/#experience-data","title":"Experience Data","text":"<p>The experience data generated during selfplay are stored in a simple raw binary format with accompanying metadata in json.  This makes it possible to efficiently load and take subsets of the data in Python during training using <code>numpy.memmap</code>.</p> <p>For each game, the experience data include:</p> <ul> <li>A tensor describing the input state before each move.</li> <li>A tensor describing the MCTS visit counts associated with each move (used as the   target for the policy network).</li> <li>A tensor describing the reward (game outcome) associated with each move.</li> </ul>"},{"location":"design/neural-network/#training","title":"Training","text":"<p>Training updates follow the approach outlined in the original AlphaZero work: the network weights are updated using gradient descent based on a loss function that sums over mean-squared error and cross-entropy losses.  In other words, the network's value output is trained to predict the game outcome measure, which is defined as -1 for a loss, 0 for a draw, and 1 for a win.  The network's policy output is trained to predict the Monte Carlo tree search visit probabilities.  The training code is found here: train.py.</p> <p>Because training updates are implemented using PyTorch, training run sequences that iterate between selfplay and training updates retain two versions of the network data: (1) a PyTorch model state dict for use in subsequent training updates, and (2) an ONNX export for use in the C++ code.</p> <ol> <li> <p>Earlier versions of dlchess used an encoding with 21 input planes, which did not include an en passant plane.\u00a0\u21a9</p> </li> </ol>"},{"location":"design/uci/","title":"UCI (Universal Chess Interface)","text":"<p>The Universal Chess Interface, known as UCI, is the standard interface for communicating with a chess engine.  This is used by chess GUIs, programs for managing tournament play between chess engines, connecting bots online, and more.  Here we review the basics of how this works and how it is implemented in <code>dlchess</code>.</p> <p>The basic concept is that all communication is done via standard input and output using text commands.  During the course of a game, the process managing the game (e.g., the chess GUI) will send a message about the current game state, followed by a message telling the chess enging to find the best move.  The chess engine processes this information, performs its search, and then responds with a message indicating the selected move.  Of course, there are some additional details for providing the chess engine with information about the time clocks, setting options, and more.</p> <p>In <code>dlchess</code>, the AlphaZero search algorithm is implemented against a simple abstract interface, which we refer to as an \"Agent\".  At a minimum, the Agent must provide a method for selecting a move when shown a game state.  In the following example, we also provide a method for the agent to configure itself based on the state of the clock.  As discussed in Chess Framework, the <code>Board</code> structure represents the game state.</p> Agent interface<pre><code>class Agent {\n public:\n  virtual chess::Move select_move(const chess::Board&amp;) = 0;\n  virtual void set_search_time(std::optional&lt;int&gt; move_time_ms,\n                               std::optional&lt;int&gt; time_left_ms,\n                               std::optional&lt;int&gt; inc_ms,\n                               const chess::Board&amp; b) {}\n};\n</code></pre> <p>UCI communication can then be managed via a loop that repeatedly polls standard input for commands and then handles them accordingly.  The following example shows handling of of a subset of the UCI commands, where the corresponding actions are implemented via separate functions.  The most important commands are <code>\"position\"</code> and <code>\"go\"</code>.  The <code>\"position\"</code> command is used to supply information about the current game state.  This is done through a combination of a FEN string and an optional sequence of moves.  During a game, a chess GUI will typically represent each position by providing the starting FEN followed by a list of all subsequent moves, so that the engine has access to the game history.  Note that we handle the <code>\"position\"</code> command by calling the <code>parse_pos</code> function, which returns a <code>Board</code> instance.  Then, the <code>\"go\"</code> command is handled by the <code>parse_go</code> function, where we provide a copy of the current UCI line (which may contain additional parameters after the \"go\" command), the <code>Board</code> instance, and the <code>agent</code> pointer.  A portion of the implementation of the <code>parse_go</code> function is shown below, where we see that the chess engine responds with the <code>\"bestmove\"</code> command, to communicate the selected move back to the process that is managing the game.</p> <pre><code>void uci_loop(zero::Agent* agent) {\n  auto b = chess::Board();\n\n  agent-&gt;info.game_mode = zero::GameMode::uci;\n\n  uci_ok();\n\n  std::string input;\n\n  while (true) {\n    std::cout &lt;&lt; std::flush;\n\n    std::getline(std::cin, input);\n\n    if (input[0] == '\\n')\n      continue;\n    else if (input.starts_with(\"isready\"))\n      std::cout &lt;&lt; \"readyok\" &lt;&lt; std::endl;\n    else if (input.starts_with(\"position\"))\n      b = parse_pos(input);\n    else if (input.starts_with(\"ucinewgame\"))\n      b = parse_pos(\"position startpos\\n\");\n    else if (input.starts_with(\"setoption\"))\n      parse_setoption(input, agent);\n    else if (input.starts_with(\"go\"))\n      parse_go(input, b, agent);\n    else if (input.starts_with(\"quit\"))\n      break;\n  }\n};\n</code></pre> <pre><code>void parse_go(std::string&amp; line, const chess::Board&amp; b, Agent* agent) {\n  ...\n\n  agent-&gt;set_search_time(move_time_ms, time_left_ms, inc_ms, b);\n  auto mv = agent-&gt;select_move(b);\n  std::cout &lt;&lt; \"bestmove \" &lt;&lt; mv &lt;&lt; std::endl;\n}\n</code></pre>"}]}